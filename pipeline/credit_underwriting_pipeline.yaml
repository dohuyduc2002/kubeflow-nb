# PIPELINE DEFINITION
# Name: credit-underwriting-training-pipeline
# Description: End-to-end underwriting flow (MinIO ➜ preprocess ➜ train ➜ MLflow)
# Inputs:
#    aws_access_key_id: str [Default: 'minio']
#    aws_secret_access_key: str [Default: 'minio123']
#    bucket_name: str [Default: 'sample-data']
#    endpoint_url: str [Default: 'http://minio.minio.svc.cluster.local:9000']
#    experiment_name: str [Default: 'demo-kubeflow']
#    mlflow_tracking_uri: str [Default: 'http://mlflow.mlflow.svc.cluster.local:5000']
#    model_name: str [Default: 'lgbm']
#    target_col: str [Default: 'TARGET']
#    test_key: str [Default: 'data/application_test.csv']
#    train_key: str [Default: 'data/application_train.csv']
#    version: str [Default: 'v1']
components:
  comp-dataloader-component:
    executorLabel: exec-dataloader-component
    inputDefinitions:
      parameters:
        aws_access_key_id:
          defaultValue: minio
          isOptional: true
          parameterType: STRING
        aws_secret_access_key:
          defaultValue: minio123
          isOptional: true
          parameterType: STRING
        bucket_name:
          defaultValue: sample-data
          isOptional: true
          parameterType: STRING
        endpoint_url:
          defaultValue: http://minio.minio.svc.cluster.local:9000
          isOptional: true
          parameterType: STRING
        test_key:
          defaultValue: data/application_test.csv
          isOptional: true
          parameterType: STRING
        train_key:
          defaultValue: data/application_train.csv
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        test_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-detect-feature-types-component:
    executorLabel: exec-detect-feature-types-component
    inputDefinitions:
      artifacts:
        train_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        categorical_cols_json:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        numerical_cols_json:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-modeling-component:
    executorLabel: exec-modeling-component
    inputDefinitions:
      artifacts:
        processed_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        processed_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        experiment_name:
          defaultValue: demo-kubeflow
          isOptional: true
          parameterType: STRING
        mlflow_access_key_id:
          defaultValue: minio
          isOptional: true
          parameterType: STRING
        mlflow_endpoint_url:
          defaultValue: http://minio.minio.svc.cluster.local:9000
          isOptional: true
          parameterType: STRING
        mlflow_secret_access_key:
          defaultValue: minio123
          isOptional: true
          parameterType: STRING
        mlflow_tracking_uri:
          defaultValue: http://mlflow.mlflow.svc.cluster.local:5000
          isOptional: true
          parameterType: STRING
        model_name:
          defaultValue: xgb
          isOptional: true
          parameterType: STRING
        version:
          defaultValue: v1
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model_joblib:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        registered_uri:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-preprocess-binning-component:
    executorLabel: exec-preprocess-binning-component
    inputDefinitions:
      artifacts:
        categorical_cols_json:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        numerical_cols_json:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        test_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        data_version:
          defaultValue: v1
          isOptional: true
          parameterType: STRING
        target_col:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        processed_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        processed_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        transformer_joblib:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-dataloader-component:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - dataloader_component
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef dataloader_component(\n    test_output: dsl.Output[dsl.Dataset],\n\
          \    train_output: dsl.Output[dsl.Dataset],\n\n    endpoint_url: str = \"\
          http://minio.minio.svc.cluster.local:9000\",\n    bucket_name: str  = \"\
          sample-data\",\n    test_key: str     = \"data/application_test.csv\",\n\
          \    train_key: str    = \"data/application_train.csv\",\n    aws_access_key_id:\
          \ str = \"minio\",\n    aws_secret_access_key: str = \"minio123\",\n):\n\
          \    # All non-KFP imports are inside the function\n    import boto3\n \
          \   import pandas as pd\n    import io\n\n    # Connect to MinIO / S3-compatible\
          \ storage\n    s3 = boto3.client(\n        \"s3\",\n        endpoint_url=endpoint_url,\n\
          \        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n\
          \    )\n\n    # Download and save test set\n    obj_test = s3.get_object(Bucket=bucket_name,\
          \ Key=test_key)\n    test_df = pd.read_csv(io.BytesIO(obj_test[\"Body\"\
          ].read()))\n    test_df.to_csv(test_output.path, index=False)\n\n    # Download\
          \ and save train set\n    obj_train = s3.get_object(Bucket=bucket_name,\
          \ Key=train_key)\n    train_df = pd.read_csv(io.BytesIO(obj_train[\"Body\"\
          ].read()))\n    train_df.to_csv(train_output.path, index=False)\n\n"
        image: microwave1005/scipy-img:latest
    exec-detect-feature-types-component:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - detect_feature_types_component
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'jsonschema'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef detect_feature_types_component(\n    train_csv: dsl.Input[dsl.Dataset],\n\
          \    categorical_cols_json: dsl.Output[dsl.Artifact],\n    numerical_cols_json:\
          \ dsl.Output[dsl.Artifact],\n):\n    \"\"\"\n    Reads the training CSV\
          \ and outputs two JSON files:\n    1. categorical_cols.json\n    2. numerical_cols.json\n\
          \    \"\"\"\n\n    # All non-KFP imports live inside the function\n    import\
          \ pandas as pd\n    import json\n\n    # ------------------------------------------------------------------\n\
          \    #  Load training data\n    # ------------------------------------------------------------------\n\
          \    df = pd.read_csv(train_csv.path)\n\n    # Identify column types\n \
          \   numerical_features = df.select_dtypes(include=[\"int64\", \"float64\"\
          ]).columns.tolist()\n    categorical_features = df.select_dtypes(include=[\"\
          object\"]).columns.tolist()\n\n    # Remove identifiers / target\n    for\
          \ col in [\"SK_ID_CURR\", \"TARGET\"]:\n        if col in numerical_features:\n\
          \            numerical_features.remove(col)\n        if col in categorical_features:\n\
          \            categorical_features.remove(col)\n\n    # ------------------------------------------------------------------\n\
          \    #  Persist lists as JSON\n    # ------------------------------------------------------------------\n\
          \    with open(categorical_cols_json.path, \"w\") as f_cat, open(\n    \
          \    numerical_cols_json.path, \"w\"\n    ) as f_num:\n        json.dump(categorical_features,\
          \ f_cat, indent=2)\n        json.dump(numerical_features, f_num, indent=2)\n\
          \n    # Optional console output for debug\n    print(\"Categorical features:\"\
          , len(categorical_features))\n    print(categorical_features)\n    print(\"\
          \\nNumerical features:\", len(numerical_features))\n    print(numerical_features)\n\
          \n"
        image: microwave1005/scipy-img:latest
    exec-modeling-component:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - modeling_component
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'mlflow' 'optuna'\
          \ 'shap' 'loguru' 'lightgbm' 'xgboost' 'joblib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef modeling_component(\n    processed_train: dsl.Input[dsl.Dataset],\n\
          \    processed_test:  dsl.Input[dsl.Dataset],\n    registered_uri:  dsl.Output[dsl.Artifact],\n\
          \    model_joblib:    dsl.Output[dsl.Artifact],\n\n    model_name:     \
          \ str = \"xgb\",    # \"xgb\" | \"lgbm\"\n    version:         str = \"\
          v1\",\n    experiment_name: str = \"demo-kubeflow\",\n\n    mlflow_endpoint_url:\
          \     str = \"http://minio.minio.svc.cluster.local:9000\",\n    mlflow_access_key_id:\
          \    str = \"minio\",\n    mlflow_secret_access_key:str = \"minio123\",\n\
          \    mlflow_tracking_uri:     str = \"http://mlflow.mlflow.svc.cluster.local:5000\"\
          ,\n):\n    \"\"\"\n    Train an XGBoost / LightGBM model with Optuna tuning\
          \ on a held-out split,\n    refit on full data, then log accuracy, SHAP\
          \ plot, and model to MLflow.\n    \"\"\"\n    import os\n    import os.path\
          \ as osp\n    from tempfile import NamedTemporaryFile\n\n    import joblib,\
          \ optuna, mlflow, shap\n    import matplotlib.pyplot as plt\n    import\
          \ pandas as pd\n    import xgboost as xgb\n    from lightgbm import LGBMClassifier\n\
          \    from sklearn.model_selection import train_test_split\n    from sklearn.metrics\
          \ import accuracy_score\n    from loguru import logger\n\n    # \u2500\u2500\
          \u2500 MLflow / MinIO setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"\
          ] = mlflow_endpoint_url\n    os.environ[\"AWS_ACCESS_KEY_ID\"]      = mlflow_access_key_id\n\
          \    os.environ[\"AWS_SECRET_ACCESS_KEY\"]  = mlflow_secret_access_key\n\
          \    mlflow.set_tracking_uri(mlflow_tracking_uri)\n    mlflow.set_experiment(experiment_name)\n\
          \    logger.info(\"MLflow experiment = {}\", experiment_name)\n\n    # \u2500\
          \u2500\u2500 load full train data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    df = pd.read_csv(processed_train.path)\n\
          \    y_full = df[\"TARGET\"].values\n    X_full = df.drop(columns=[\"TARGET\"\
          ])\n\n    # \u2500\u2500\u2500 split off a validation set for Optuna \u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    X_tune, X_val, y_tune, y_val\
          \ = train_test_split(\n        X_full, y_full, test_size=0.2, random_state=42,\
          \ stratify=y_full\n    )\n\n    # \u2500\u2500\u2500 Optuna hyper-parameter\
          \ tuning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\n    def suggest(trial):\n        return\
          \ {\n            \"max_depth\":        trial.suggest_int(\"max_depth\",\
          \ 2, 8),\n            \"learning_rate\":    trial.suggest_float(\"learning_rate\"\
          , 1e-3, 0.3, log=True),\n            \"n_estimators\":     trial.suggest_int(\"\
          n_estimators\", 100, 500),\n            \"subsample\":        trial.suggest_float(\"\
          subsample\", 0.5, 1.0),\n            \"colsample_bytree\": trial.suggest_float(\"\
          colsample_bytree\", 0.5, 1.0),\n        }\n\n    def objective_xgb(trial):\n\
          \        m = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"auc\"\
          , **suggest(trial))\n        m.fit(X_tune, y_tune)\n        preds = m.predict(X_val)\n\
          \        return accuracy_score(y_val, preds)\n\n    def objective_lgbm(trial):\n\
          \        m = LGBMClassifier(**suggest(trial))\n        m.fit(X_tune, y_tune)\n\
          \        preds = m.predict(X_val)\n        return accuracy_score(y_val,\
          \ preds)\n\n    logger.info(\"Running Optuna tuning\")\n    study = optuna.create_study(direction=\"\
          maximize\")\n    study.optimize(\n        objective_xgb if model_name.lower()\
          \ == \"xgb\" else objective_lgbm,\n        n_trials=10,\n        show_progress_bar=False,\n\
          \    )\n    best_params = study.best_params\n    logger.success(\"Best params:\
          \ {}\", best_params)\n\n    # \u2500\u2500\u2500 final fit on full data\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \n    model = (\n        xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"\
          auc\", **best_params)\n        if model_name.lower() == \"xgb\"\n      \
          \  else LGBMClassifier(**best_params)\n    )\n    model.fit(X_full, y_full)\n\
          \n    # \u2500\u2500\u2500 prepare evaluation set \u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    X_eval, y_eval = X_full,\
          \ y_full\n    # if an external test with labels exists, use it instead\n\
          \    if processed_test.path and osp.exists(processed_test.path):\n     \
          \   df_test = pd.read_csv(processed_test.path)\n        if \"TARGET\" in\
          \ df_test.columns and not df_test.empty:\n            y_eval = df_test[\"\
          TARGET\"].values\n            X_eval = df_test.drop(columns=[\"TARGET\"\
          ])\n\n    # \u2500\u2500\u2500 compute accuracy \u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\n    preds = model.predict(X_eval)\n    acc   = accuracy_score(y_eval,\
          \ preds)\n\n    # \u2500\u2500\u2500 save SHAP summary plot \u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    with NamedTemporaryFile(delete=False,\
          \ suffix=\"_shap.png\") as f:\n        shap_values = shap.Explainer(model)(X_eval)\n\
          \        shap.summary_plot(shap_values, X_eval, show=False)\n        plt.gcf().savefig(f.name)\n\
          \        shap_path = f.name\n    plt.close()\n\n    # \u2500\u2500\u2500\
          \ save model artifact \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\n    joblib.dump(model, model_joblib.path)\n\
          \n    # \u2500\u2500\u2500 log to MLflow \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\n    run_tag  = \"XGBoost\" if model_name.lower() == \"xgb\"\
          \ else \"LightGBM\"\n    run_name = f\"{version}_{run_tag}\"\n\n    with\
          \ mlflow.start_run(run_name=f\"{run_name}_run\") as run:\n        mlflow.log_params(best_params)\n\
          \        mlflow.log_metric(\"accuracy\", acc)\n        mlflow.log_artifact(shap_path,\
          \       artifact_path=\"model_artifacts\")\n        mlflow.log_artifact(model_joblib.path,\
          \ artifact_path=\"model_artifacts\")\n        if model_name.lower() == \"\
          xgb\":\n            mlflow.xgboost.log_model(model, artifact_path=\"flavour\"\
          )\n        else:\n            mlflow.lightgbm.log_model(model, artifact_path=\"\
          flavour\")\n        uri = mlflow.get_artifact_uri(\"model_artifacts\")\n\
          \        mlflow.register_model(uri, run_name)\n\n    # \u2500\u2500\u2500\
          \ write registered URI for downstream steps \u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    with open(registered_uri.path,\
          \ \"w\") as f_out:\n        f_out.write(uri)\n\n    logger.success(\"Model\
          \ training, logging, and registration complete\")\n\n"
        image: microwave1005/scipy-img:latest
    exec-preprocess-binning-component:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess_binning_component
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess_binning_component(\n    train_csv: dsl.Input[dsl.Dataset],\n\
          \    test_csv : dsl.Input[dsl.Dataset],\n    categorical_cols_json: dsl.Input[dsl.Artifact],\n\
          \    numerical_cols_json : dsl.Input[dsl.Artifact],\n    processed_train:\
          \ dsl.Output[dsl.Dataset],\n    processed_test : dsl.Output[dsl.Dataset],\n\
          \    transformer_joblib: dsl.Output[dsl.Artifact],\n    target_col: str,\n\
          \    data_version: str = \"v1\",\n):\n\n    import pandas as pd\n    import\
          \ numpy as np\n    import json, joblib\n    from pathlib import Path\n \
          \   from optbinning import BinningProcess\n    from sklearn.feature_selection\
          \ import SequentialFeatureSelector\n    from xgboost import XGBClassifier\n\
          \    from tqdm import tqdm\n    from loguru import logger\n\n    df_train\
          \ = pd.read_csv(train_csv.path)\n    df_test  = pd.read_csv(test_csv.path)\n\
          \n    with open(categorical_cols_json.path) as f_cat, open(numerical_cols_json.path)\
          \ as f_num:\n        categorical_cols = json.load(f_cat)\n        numerical_cols\
          \   = json.load(f_num)\n\n    # ---------- build X / y ----------\n    X_train\
          \ = df_train.drop(columns=[target_col])\n    y_train = df_train[target_col]\n\
          \n    if target_col in df_test.columns:\n        X_test = df_test.drop(columns=[target_col])\n\
          \    else:\n        X_test = df_test.copy()\n\n    # ------------------------------------------------------------------\n\
          \    #   Helper classes (trimmed but identical logic to your code)\n   \
          \ # ------------------------------------------------------------------\n\
          \    class Preprocess:\n        def __init__(self, X_tr, X_te, y_tr, cat_cols,\
          \ num_cols):\n            self.X_train_orig = X_tr.copy()\n            self.X_test_orig\
          \  = X_te.copy()\n            self.X_train      = X_tr.copy()\n        \
          \    self.X_test       = X_te.copy()\n            self.y_train      = y_tr\n\
          \            self.categorical_cols = cat_cols\n            self.numerical_cols\
          \   = num_cols\n            self.X_train_processed = None\n            self.X_test_processed\
          \  = None\n            self.binning_process   = None\n\n        def impute_data(self):\n\
          \            for col in self.numerical_cols:\n                med = self.X_train[col].median()\n\
          \                self.X_train[col].fillna(med, inplace=True)\n         \
          \       if col in self.X_test.columns:\n                    self.X_test[col].fillna(med,\
          \ inplace=True)\n\n            for col in self.categorical_cols:\n     \
          \           mode_val = self.X_train[col].mode()\n                fill_val\
          \ = mode_val[0] if not mode_val.empty else \"missing\"\n               \
          \ self.X_train[col].fillna(fill_val, inplace=True)\n                if col\
          \ in self.X_test.columns:\n                    self.X_test[col].fillna(fill_val,\
          \ inplace=True)\n\n        def run(self):\n            self.impute_data()\n\
          \            all_feats = self.categorical_cols + self.numerical_cols\n \
          \           self.binning_process = BinningProcess(\n                variable_names=all_feats,\
          \ categorical_variables=self.categorical_cols\n            )\n         \
          \   self.binning_process.fit(self.X_train[all_feats].to_numpy(), self.y_train)\n\
          \            self.X_train_processed = pd.DataFrame(\n                self.binning_process.transform(self.X_train[all_feats].to_numpy()),\n\
          \                columns=all_feats,\n            )\n            if not self.X_test.empty:\n\
          \                self.X_test_processed = pd.DataFrame(\n               \
          \     self.binning_process.transform(self.X_test[all_feats].to_numpy()),\n\
          \                    columns=all_feats,\n                )\n           \
          \ else:\n                self.X_test_processed = pd.DataFrame(columns=all_feats)\n\
          \            return self.X_train_processed, self.X_test_processed\n\n  \
          \      @staticmethod\n        def compute_iv(series, y):\n            df\
          \ = pd.DataFrame({\"bin\": series, \"target\": y})\n            tot_gd =\
          \ (df[\"target\"] == 0).sum()\n            tot_bd = (df[\"target\"] == 1).sum()\n\
          \            iv, eps = 0, 0.5\n            for _, grp in df.groupby(\"bin\"\
          ):\n                gd = (grp[\"target\"] == 0).sum() or eps\n         \
          \       bd = (grp[\"target\"] == 1).sum() or eps\n                iv +=\
          \ ((gd / tot_gd) - (bd / tot_bd)) * np.log((gd / tot_gd) / (bd / tot_bd))\n\
          \            return iv\n\n        def filter_features(self):\n         \
          \   excl, iv_dict = [], {}\n            for col in tqdm(self.X_train_processed.columns,\
          \ desc=\"Filtering\"):\n                iv_val = Preprocess.compute_iv(self.X_train_processed[col],\
          \ self.y_train)\n                iv_dict[col] = iv_val\n               \
          \ miss = self.X_train_orig[col].isnull().mean()\n                if iv_val\
          \ > 0.5 or iv_val < 0.02 or miss > 0.10:\n                    excl.append(col)\n\
          \            self.X_train_processed.drop(columns=excl, inplace=True)\n \
          \           self.X_test_processed .drop(columns=excl, errors=\"ignore\"\
          , inplace=True)\n            return iv_dict, excl\n\n    logger.info(\"\
          \ Preprocessing \u2026\")\n    pre = Preprocess(X_train, X_test, y_train,\
          \ categorical_cols, numerical_cols)\n    X_train_proc, X_test_proc = pre.run()\n\
          \    iv_dict, iv_excl = pre.filter_features()\n\n    logger.info(\" Sequential\
          \ Feature Selection \u2026\")\n    sfs = SequentialFeatureSelector(\n  \
          \      XGBClassifier(eval_metric=\"auc\"),\n        n_features_to_select=\"\
          auto\",\n        direction=\"forward\",\n    )\n    sfs.fit(X_train_proc,\
          \ y_train)\n    selected_cols = list(X_train_proc.columns[sfs.get_support()])\n\
          \n    final_train = pd.DataFrame(sfs.transform(X_train_proc), columns=selected_cols)\n\
          \    final_train[target_col] = y_train\n\n    final_test = (\n        pd.DataFrame(sfs.transform(X_test_proc),\
          \ columns=selected_cols)\n        if not X_test_proc.empty else pd.DataFrame(columns=selected_cols)\n\
          \    )\n\n    # ------------------------------------------------------------------\n\
          \    #  Write outputs\n    # ------------------------------------------------------------------\n\
          \    final_train.to_csv(processed_train.path, index=False)\n    final_test\
          \ .to_csv(processed_test.path , index=False)\n\n    # Save the fitted artefacts\n\
          \    joblib.dump(\n        {\n            \"binning_process\": pre.binning_process,\n\
          \            \"sfs_selector\"  : sfs,\n            \"selected_cols\" : selected_cols,\n\
          \            \"iv_excluded\"   : iv_excl,\n        },\n        transformer_joblib.path,\n\
          \    )\n\n    logger.info(\" Preprocessing + binning complete.\")\n\n"
        image: microwave1005/scipy-img:latest
pipelineInfo:
  description: "End-to-end underwriting flow (MinIO \u279C preprocess \u279C train\
    \ \u279C MLflow)"
  name: credit-underwriting-training-pipeline
root:
  dag:
    tasks:
      dataloader-component:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-dataloader-component
        inputs:
          parameters:
            aws_access_key_id:
              componentInputParameter: aws_access_key_id
            aws_secret_access_key:
              componentInputParameter: aws_secret_access_key
            bucket_name:
              componentInputParameter: bucket_name
            endpoint_url:
              componentInputParameter: endpoint_url
            test_key:
              componentInputParameter: test_key
            train_key:
              componentInputParameter: train_key
        taskInfo:
          name: dataloader-component
      detect-feature-types-component:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-detect-feature-types-component
        dependentTasks:
        - dataloader-component
        inputs:
          artifacts:
            train_csv:
              taskOutputArtifact:
                outputArtifactKey: train_output
                producerTask: dataloader-component
        taskInfo:
          name: detect-feature-types-component
      modeling-component:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-modeling-component
        dependentTasks:
        - preprocess-binning-component
        inputs:
          artifacts:
            processed_test:
              taskOutputArtifact:
                outputArtifactKey: processed_test
                producerTask: preprocess-binning-component
            processed_train:
              taskOutputArtifact:
                outputArtifactKey: processed_train
                producerTask: preprocess-binning-component
          parameters:
            experiment_name:
              componentInputParameter: experiment_name
            mlflow_access_key_id:
              componentInputParameter: aws_access_key_id
            mlflow_endpoint_url:
              componentInputParameter: endpoint_url
            mlflow_secret_access_key:
              componentInputParameter: aws_secret_access_key
            mlflow_tracking_uri:
              componentInputParameter: mlflow_tracking_uri
            model_name:
              componentInputParameter: model_name
            version:
              componentInputParameter: version
        taskInfo:
          name: modeling-component
      preprocess-binning-component:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess-binning-component
        dependentTasks:
        - dataloader-component
        - detect-feature-types-component
        inputs:
          artifacts:
            categorical_cols_json:
              taskOutputArtifact:
                outputArtifactKey: categorical_cols_json
                producerTask: detect-feature-types-component
            numerical_cols_json:
              taskOutputArtifact:
                outputArtifactKey: numerical_cols_json
                producerTask: detect-feature-types-component
            test_csv:
              taskOutputArtifact:
                outputArtifactKey: test_output
                producerTask: dataloader-component
            train_csv:
              taskOutputArtifact:
                outputArtifactKey: train_output
                producerTask: dataloader-component
          parameters:
            data_version:
              componentInputParameter: version
            target_col:
              componentInputParameter: target_col
        taskInfo:
          name: preprocess-binning-component
  inputDefinitions:
    parameters:
      aws_access_key_id:
        defaultValue: minio
        isOptional: true
        parameterType: STRING
      aws_secret_access_key:
        defaultValue: minio123
        isOptional: true
        parameterType: STRING
      bucket_name:
        defaultValue: sample-data
        isOptional: true
        parameterType: STRING
      endpoint_url:
        defaultValue: http://minio.minio.svc.cluster.local:9000
        isOptional: true
        parameterType: STRING
      experiment_name:
        defaultValue: demo-kubeflow
        isOptional: true
        parameterType: STRING
      mlflow_tracking_uri:
        defaultValue: http://mlflow.mlflow.svc.cluster.local:5000
        isOptional: true
        parameterType: STRING
      model_name:
        defaultValue: lgbm
        isOptional: true
        parameterType: STRING
      target_col:
        defaultValue: TARGET
        isOptional: true
        parameterType: STRING
      test_key:
        defaultValue: data/application_test.csv
        isOptional: true
        parameterType: STRING
      train_key:
        defaultValue: data/application_train.csv
        isOptional: true
        parameterType: STRING
      version:
        defaultValue: v1
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.12.1
