# PIPELINE DEFINITION
# Name: underwritingworkflow
# Description: Download raw → preprocess → download processed → train & register
# Inputs:
#    bucket_name: str
#    data_version: str
#    dest_test_object: str
#    dest_train_object: str
#    experiment_name: str
#    minio_access_key: str
#    minio_endpoint: str
#    minio_secret_key: str
#    mlflow_endpoint: str
#    model_name: str
#    n_features_to_select: str
#    parent_run_name: str
#    raw_test_object: str
#    raw_train_object: str
#    suffix: str
components:
  comp-dataloader:
    executorLabel: exec-dataloader
    inputDefinitions:
      parameters:
        bucket_name:
          parameterType: STRING
        minio_access_key:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
        object_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-dataloader-2:
    executorLabel: exec-dataloader-2
    inputDefinitions:
      parameters:
        bucket_name:
          parameterType: STRING
        minio_access_key:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
        object_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-modeling:
    executorLabel: exec-modeling
    inputDefinitions:
      artifacts:
        mlflow_run_id:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        test_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        experiment_name:
          parameterType: STRING
        minio_access_key:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
        mlflow_endpoint:
          parameterType: STRING
        model_name:
          parameterType: STRING
        suffix:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        model_joblib:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        registered_model:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-preprocess:
    executorLabel: exec-preprocess
    inputDefinitions:
      artifacts:
        test_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        data_version:
          parameterType: STRING
        dest_test_object:
          parameterType: STRING
        dest_train_object:
          parameterType: STRING
        experiment_name:
          parameterType: STRING
        minio_access_key:
          parameterType: STRING
        minio_endpoint:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
        mlflow_endpoint:
          parameterType: STRING
        n_features_to_select:
          parameterType: STRING
        parent_run_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        mlflow_run_id:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        output_test_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        output_train_csv:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        transformer_joblib:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-dataloader:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - dataloader
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef dataloader(\n    minio_endpoint: str,\n    minio_access_key:\
          \ str,\n    minio_secret_key: str,\n    bucket_name: str,\n    object_name:\
          \ str,\n    output: Output[Dataset],   \n):\n    \"\"\"\n    Download a\
          \ single object from MinIO into a KFP Dataset artifact.\n    \"\"\"\n  \
          \  from minio import Minio\n    import os\n\n\n    os.makedirs(os.path.dirname(output.path),\
          \ exist_ok=True)\n    client = Minio(\n        minio_endpoint,\n       \
          \ access_key=minio_access_key,\n        secret_key=minio_secret_key,\n \
          \       secure=False,\n    )\n    client.fget_object(bucket_name, object_name,\
          \ output.path)\n    print(f\"Downloaded {object_name} to {output.path}\"\
          )\n\n"
        image: microwave1005/scipy-img:latest
    exec-dataloader-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - dataloader
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef dataloader(\n    minio_endpoint: str,\n    minio_access_key:\
          \ str,\n    minio_secret_key: str,\n    bucket_name: str,\n    object_name:\
          \ str,\n    output: Output[Dataset],   \n):\n    \"\"\"\n    Download a\
          \ single object from MinIO into a KFP Dataset artifact.\n    \"\"\"\n  \
          \  from minio import Minio\n    import os\n\n\n    os.makedirs(os.path.dirname(output.path),\
          \ exist_ok=True)\n    client = Minio(\n        minio_endpoint,\n       \
          \ access_key=minio_access_key,\n        secret_key=minio_secret_key,\n \
          \       secure=False,\n    )\n    client.fget_object(bucket_name, object_name,\
          \ output.path)\n    print(f\"Downloaded {object_name} to {output.path}\"\
          )\n\n"
        image: microwave1005/scipy-img:latest
    exec-modeling:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - modeling
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef modeling(\n    train_csv: Input[Dataset],\n    test_csv: Input[Dataset],\n\
          \    model_joblib: Output[Artifact],\n    registered_model: Output[Artifact],\n\
          \    mlflow_run_id: Input[Artifact],\n    minio_endpoint: str,\n    minio_access_key:\
          \ str,\n    minio_secret_key: str,\n    mlflow_endpoint: str,\n    experiment_name:\
          \ str,\n    model_name: str,\n    suffix: str,\n):\n    import os, json,\
          \ optuna, shap, joblib, matplotlib.pyplot as plt\n    import pandas as pd\n\
          \    from pathlib import Path\n    import mlflow, xgboost as xgb\n    from\
          \ lightgbm import LGBMClassifier\n    from sklearn.model_selection import\
          \ train_test_split\n    from sklearn.metrics import accuracy_score, classification_report\n\
          \n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = f\"http://{minio_endpoint}\"\
          \n    os.environ[\"AWS_ACCESS_KEY_ID\"] = minio_access_key\n    os.environ[\"\
          AWS_SECRET_ACCESS_KEY\"] = minio_secret_key\n    os.environ[\"MLFLOW_ENDPOINT\"\
          ] = f\"http://{mlflow_endpoint}\"\n\n    mlflow.set_tracking_uri(f\"http://{mlflow_endpoint}\"\
          )\n    mlflow.set_experiment(experiment_name)\n\n    parent_id: str = Path(mlflow_run_id.path).read_text().strip()\n\
          \n    df = pd.read_csv(train_csv.path)\n    X, y = df.drop(\"TARGET\", axis=1),\
          \ df[\"TARGET\"]\n\n    mlflow.end_run()\n    with mlflow.start_run(run_id=parent_id):\n\
          \n        def objective(trial):\n            params = {\n              \
          \  \"max_depth\": trial.suggest_int(\"max_depth\", 2, 8),\n            \
          \    \"learning_rate\": trial.suggest_float(\n                    \"learning_rate\"\
          , 1e-3, 0.3, log=True\n                ),\n                \"n_estimators\"\
          : trial.suggest_int(\"n_estimators\", 100, 300),\n                \"subsample\"\
          : trial.suggest_float(\"subsample\", 0.5, 1.0),\n                \"colsample_bytree\"\
          : trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n            }\n\n\
          \            X_train, X_val, y_train, y_val = train_test_split(\n      \
          \          X, y, test_size=0.2, random_state=42\n            )\n\n     \
          \       clf = (\n                xgb.XGBClassifier(eval_metric=\"auc\",\
          \ **params)\n                if model_name == \"xgb\"\n                else\
          \ LGBMClassifier(**params)\n            )\n\n            # ----- Hyperparameter\
          \ optimization -----\n            with mlflow.start_run(\n             \
          \   nested=True, run_name=f\"optuna_{trial.number}\"\n            ) as trial_run:\n\
          \                clf.fit(X_train, y_train)\n                acc = accuracy_score(y_val,\
          \ clf.predict(X_val))\n\n                mlflow.log_params(params)\n   \
          \             mlflow.log_metric(\"accuracy\", acc)\n\n                #\
          \ log model\n                if model_name == \"xgb\":\n               \
          \     mlflow.xgboost.log_model(clf, artifact_path=\"model\")\n         \
          \       else:\n                    mlflow.lightgbm.log_model(clf, artifact_path=\"\
          model\")\n\n                # root artifact dir\n                art_dir\
          \ = \"/tmp/trial_artifacts\"\n                os.makedirs(art_dir, exist_ok=True)\n\
          \n                expl = shap.Explainer(clf)\n                shap_vals\
          \ = expl(X_val)\n                plt.figure()\n                shap.summary_plot(shap_vals,\
          \ X_val, show=False)\n                shap_path = f\"{art_dir}/shap.png\"\
          \n                plt.savefig(shap_path)\n                plt.close()\n\n\
          \                report_txt = f\"{art_dir}/report.txt\"\n              \
          \  report = classification_report(y_val, clf.predict(X_val))\n         \
          \       Path(report_txt).write_text(report)\n\n                joblib_path\
          \ = f\"{art_dir}/model_{trial.number}.joblib\"\n                joblib.dump(clf,\
          \ joblib_path)\n\n                mlflow.log_artifacts(art_dir, artifact_path=\"\
          trial_artifacts\")\n\n                trial.set_user_attr(\"mlflow_run_id\"\
          , trial_run.info.run_id)\n\n            return acc\n\n        study = optuna.create_study(direction=\"\
          maximize\")\n        study.optimize(objective, n_trials=5)\n\n        best_trial\
          \ = study.best_trial\n        best_run_id = best_trial.user_attrs[\"mlflow_run_id\"\
          ]\n\n        best_model_uri = f\"runs:/{best_run_id}/model\"\n        registry\
          \ = mlflow.register_model(best_model_uri, name=f\"{model_name}_{suffix}\"\
          )\n\n    Path(registered_model.path).parent.mkdir(parents=True, exist_ok=True)\n\
          \    Path(registered_model.path).write_text(\n        json.dumps(\n    \
          \        {\n                \"parent_run\": parent_id,\n               \
          \ \"best_trial\": best_trial.number,\n                \"best_trial_run\"\
          : best_run_id,\n                \"registered\": {\"name\": registry.name,\
          \ \"version\": registry.version},\n            },\n            indent=2,\n\
          \        )\n    )\n\n"
        image: microwave1005/scipy-img:latest
    exec-preprocess:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess(\n    train_csv: Input[Dataset],\n    test_csv: Input[Dataset],\n\
          \    transformer_joblib: Output[Artifact],\n    output_train_csv: Output[Dataset],\n\
          \    output_test_csv: Output[Dataset],\n    mlflow_run_id: Output[Artifact],\n\
          \    minio_endpoint: str,\n    minio_access_key: str,\n    minio_secret_key:\
          \ str,\n    mlflow_endpoint: str,\n    parent_run_name: str,\n    dest_train_object:\
          \ str,\n    dest_test_object: str,\n    n_features_to_select: str,\n   \
          \ data_version: str,\n    experiment_name: str,\n):\n\n    import os\n \
          \   import pandas as pd\n    import numpy as np\n    import joblib\n   \
          \ from pathlib import Path\n    import mlflow\n    from optbinning import\
          \ BinningProcess\n    from sklearn.feature_selection import SelectKBest,\
          \ f_classif\n\n    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = f\"http://{minio_endpoint}\"\
          \n    os.environ[\"AWS_ACCESS_KEY_ID\"] = minio_access_key\n    os.environ[\"\
          AWS_SECRET_ACCESS_KEY\"] = minio_secret_key\n    os.environ[\"MLFLOW_ENDPOINT\"\
          ] = f\"http://{mlflow_endpoint}\"\n    # Load data\n\n    df_train = pd.read_csv(train_csv.path)\n\
          \    df_test = pd.read_csv(test_csv.path)\n\n    # Data processing functions\n\
          \    def get_lists(df):\n        numeric = df.select_dtypes(include=[\"\
          int64\", \"float64\"]).columns.tolist()\n        category = df.select_dtypes(include=[\"\
          object\"]).columns.tolist()\n        for column in (\"SK_ID_CURR\", \"TARGET\"\
          ):\n            if column in numeric:\n                numeric.remove(column)\n\
          \        return category, numeric\n\n    def iv_score(bins, y):\n      \
          \  df = pd.DataFrame({\"bins\": bins, \"target\": y})\n        total_good,\
          \ total_bad = (df.target == 0).sum(), (df.target == 1).sum()\n        score\
          \ = 0\n        for _, goods in df.groupby(\"bins\"):\n            good =\
          \ (goods.target == 0).sum() or 0.5\n            bad = (goods.target == 1).sum()\
          \ or 0.5\n            score += (good / total_good - bad / total_bad) * np.log(\n\
          \                (good / total_good) / (bad / total_bad)\n            )\n\
          \        return score\n\n    # Feature selection and binning\n    cat_cols,\
          \ num_cols = get_lists(df_train)\n    y = df_train[\"TARGET\"]\n    X_train,\
          \ X_test = df_train.drop(\"TARGET\", axis=1), df_test.copy()\n\n    survivors\
          \ = []\n    for feature in cat_cols + num_cols:\n        missing_rate =\
          \ X_train[feature].isna().mean()\n        if missing_rate > 0.1:\n     \
          \       continue  # Exclude features with more than 10% missing values\n\
          \n        # Calulate iv scoore by using quantile cut if large bin or\n \
          \       if feature in cat_cols:\n            # pd.factorize for categorical\
          \ features to label encode them\n            bins = pd.factorize(X_train[feature].fillna(\"\
          missing\"))[0]\n        else:\n            # if numeric feature has more\
          \ than is high cardinal (more than 10) use qcut, else use cut with quantile\
          \ is the nunique value\n            if X_train[feature].nunique() > 10:\n\
          \                bins = pd.qcut(\n                    X_train[feature].fillna(X_train[feature].median()),\n\
          \                    10,\n                    duplicates=\"drop\",\n   \
          \                 labels=False,\n                )\n            else:\n\
          \                bins = pd.cut(\n                    X_train[feature].fillna(X_train[feature].median()),\n\
          \                    bins=X_train[feature].nunique(),\n                \
          \    labels=False,\n                )\n\n        iv = iv_score(bins, y)\n\
          \        if 0.02 <= iv <= 0.5:\n            survivors.append(feature)\n\n\
          \    # After filtering with iv and missing rate, we can proceed with binning\n\
          \    opt_binning_process = BinningProcess(\n        variable_names=survivors,\n\
          \        categorical_variables=[col for col in survivors if col in cat_cols],\n\
          \    )\n    opt_binning_process.fit(X_train[survivors].values, y)\n    df_train_binned\
          \ = pd.DataFrame(\n        opt_binning_process.transform(X_train[survivors].values),\
          \ columns=survivors\n    )\n\n    # Due to test set does not have TARGET\
          \ col, we cannot use iv \n    df_test_binned = pd.DataFrame(\n        opt_binning_process.transform(X_test[survivors].values),\
          \ columns=survivors\n    )\n\n    # Feature selection using anova F-test\
          \ as a score function \n    k = len(survivors) if n_features_to_select ==\
          \ \"auto\" else int(n_features_to_select)\n    selector = SelectKBest(f_classif,\
          \ k=k)\n    selector.fit(df_train_binned.fillna(0), y)\n\n    keep = df_train_binned.columns[selector.get_support()]\n\
          \    out_train = pd.DataFrame(selector.transform(df_train_binned), columns=keep)\n\
          \    out_test = pd.DataFrame(selector.transform(df_train_binned), columns=keep)\n\
          \    out_train[\"TARGET\"] = y\n\n    # Save artifacts to KFP component\
          \ outputs\n    Path(transformer_joblib.path).parent.mkdir(parents=True,\
          \ exist_ok=True)\n    joblib.dump(\n        {\"opt_binning_process\": opt_binning_process,\
          \ \"selector\": selector},\n        transformer_joblib.path,\n    )\n  \
          \  out_train.to_csv(output_train_csv.path, index=False)\n    out_test.to_csv(output_test_csv.path,\
          \ index=False)\n\n    # Prepare artifact names for MLflow\n    train_artifact_name\
          \ = dest_train_object.replace(\".csv\", f\"_{data_version}.csv\")\n    test_artifact_name\
          \ = dest_test_object.replace(\".csv\", f\"_{data_version}.csv\")\n    transformer_artifact_name\
          \ = f\"transformer_{data_version}.joblib\"\n\n    # Create temporary directory\
          \ for MLflow artifacts due to mlflow requirement\n    art_dir = Path(\"\
          /tmp/mlflow_artifacts\")\n    art_dir.mkdir(parents=True, exist_ok=True)\n\
          \n    # Save artifacts to mlflow\n    joblib.dump(\n        {\"opt_binning_process\"\
          : opt_binning_process, \"selector\": selector},\n        art_dir / transformer_artifact_name,\n\
          \    )\n    out_train.to_csv(art_dir / train_artifact_name, index=False)\n\
          \    out_test.to_csv(art_dir / test_artifact_name, index=False)\n\n    #\
          \ Log to MLflow\n    mlflow.set_tracking_uri(os.environ[\"MLFLOW_ENDPOINT\"\
          ])\n    mlflow.set_experiment(experiment_name)\n\n    with mlflow.start_run(run_name=parent_run_name)\
          \ as parent:\n\n        parent_id = parent.info.run_id\n        joblib.dump(\n\
          \            {\"opt_binning_process\": opt_binning_process, \"selector\"\
          : selector},\n            transformer_joblib.path,\n        )\n        out_train.to_csv(output_train_csv.path,\
          \ index=False)\n        out_test.to_csv(output_test_csv.path, index=False)\n\
          \n        mlflow.log_artifact(transformer_joblib.path, artifact_path=\"\
          prep\")\n        mlflow.log_artifact(output_train_csv.path, artifact_path=\"\
          prep\")\n        mlflow.log_artifact(output_test_csv.path, artifact_path=\"\
          prep\")\n\n    Path(mlflow_run_id.path).parent.mkdir(parents=True, exist_ok=True)\n\
          \    Path(mlflow_run_id.path).write_text(parent_id)\n\n"
        image: microwave1005/scipy-img:latest
pipelineInfo:
  description: "Download raw \u2192 preprocess \u2192 download processed \u2192 train\
    \ & register"
  name: underwritingworkflow
root:
  dag:
    tasks:
      dataloader:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-dataloader
        inputs:
          parameters:
            bucket_name:
              componentInputParameter: bucket_name
            minio_access_key:
              componentInputParameter: minio_access_key
            minio_endpoint:
              componentInputParameter: minio_endpoint
            minio_secret_key:
              componentInputParameter: minio_secret_key
            object_name:
              componentInputParameter: raw_train_object
        taskInfo:
          name: dataloader
      dataloader-2:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-dataloader-2
        inputs:
          parameters:
            bucket_name:
              componentInputParameter: bucket_name
            minio_access_key:
              componentInputParameter: minio_access_key
            minio_endpoint:
              componentInputParameter: minio_endpoint
            minio_secret_key:
              componentInputParameter: minio_secret_key
            object_name:
              componentInputParameter: raw_test_object
        taskInfo:
          name: dataloader-2
      modeling:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-modeling
        dependentTasks:
        - preprocess
        inputs:
          artifacts:
            mlflow_run_id:
              taskOutputArtifact:
                outputArtifactKey: mlflow_run_id
                producerTask: preprocess
            test_csv:
              taskOutputArtifact:
                outputArtifactKey: output_test_csv
                producerTask: preprocess
            train_csv:
              taskOutputArtifact:
                outputArtifactKey: output_train_csv
                producerTask: preprocess
          parameters:
            experiment_name:
              componentInputParameter: experiment_name
            minio_access_key:
              componentInputParameter: minio_access_key
            minio_endpoint:
              componentInputParameter: minio_endpoint
            minio_secret_key:
              componentInputParameter: minio_secret_key
            mlflow_endpoint:
              componentInputParameter: mlflow_endpoint
            model_name:
              componentInputParameter: model_name
            suffix:
              componentInputParameter: suffix
        taskInfo:
          name: modeling
      preprocess:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess
        dependentTasks:
        - dataloader
        - dataloader-2
        inputs:
          artifacts:
            test_csv:
              taskOutputArtifact:
                outputArtifactKey: output
                producerTask: dataloader-2
            train_csv:
              taskOutputArtifact:
                outputArtifactKey: output
                producerTask: dataloader
          parameters:
            data_version:
              componentInputParameter: data_version
            dest_test_object:
              componentInputParameter: dest_test_object
            dest_train_object:
              componentInputParameter: dest_train_object
            experiment_name:
              componentInputParameter: experiment_name
            minio_access_key:
              componentInputParameter: minio_access_key
            minio_endpoint:
              componentInputParameter: minio_endpoint
            minio_secret_key:
              componentInputParameter: minio_secret_key
            mlflow_endpoint:
              componentInputParameter: mlflow_endpoint
            n_features_to_select:
              componentInputParameter: n_features_to_select
            parent_run_name:
              componentInputParameter: parent_run_name
        taskInfo:
          name: preprocess
  inputDefinitions:
    parameters:
      bucket_name:
        parameterType: STRING
      data_version:
        parameterType: STRING
      dest_test_object:
        parameterType: STRING
      dest_train_object:
        parameterType: STRING
      experiment_name:
        parameterType: STRING
      minio_access_key:
        parameterType: STRING
      minio_endpoint:
        parameterType: STRING
      minio_secret_key:
        parameterType: STRING
      mlflow_endpoint:
        parameterType: STRING
      model_name:
        parameterType: STRING
      n_features_to_select:
        parameterType: STRING
      parent_run_name:
        parameterType: STRING
      raw_test_object:
        parameterType: STRING
      raw_train_object:
        parameterType: STRING
      suffix:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.12.1
